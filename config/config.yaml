train:
  dataset_path: C:\Users\USER\Desktop\synthdataset # "dataset path"
  experiment_name: e2e_36_uph # "wandb test name || model pth name"
  fringes: # "fringe pitch list"
  - 36
  model1_name: DeepVO # for rt
  model2_name: UNet # for phase
  model3_name: CondiUNet2 # for phase jump
  resume: true # "pretrained or resume training"
  model1_path: ./saved/e2e_36_uph_rt.pth # "pretrained model1 path"
  model2_path: ./saved/e2e_36_uph_ph.pth # "pretrained model2 path"
  model3_path: ./saved/e2e_36_uph_uph.pth # "pretrained model2 path"
  view_num: 5 # "sequence length"
  batch_size: 8 # "input batch size for train"
  criterions: MSE # "criterion type"
  criterion_weights: # "loss weights for each model"
  - 1
  - 1
  - 1
  epochs: 250  # "number of epochs to train"
  log_interval: 200 # "how many batches to wait before logging training status"
  grad_accum: 1 # "grad_accumulation"
  lr: 0.001 # "learning rate"
  optimizer: AdamW # "optimizer"
  seed: 42 # "random seed"
  early_stopping: false # "early_stopping"
  patience: 25 # "early stopping patience"
  scheduler: ReduceOP # "scheduler name or false" 
  log_wandb: true # "use wandb"
  project: mvstpu # "wandb project"  
  entity: juneberm # "wandb entity"
test:
  dataset_path: C:\Users\USER\Desktop\synthdataset # "dataset path"
  model1_name: DeepVO # for rt
  model2_name: UNet # for phase
  model3_name: CondiUNet2 # for phase
  view_num: 5 # "sequence length"
  batch_size: 8 # "input batch size for test"
  fringes: # "fringe pitch list"
  - 36
  model1_path: ./saved/e2e_36_uph_rt.pth # "pretrained model1 path"
  model2_path: ./saved/e2e_36_uph_ph.pth # "pretrained model2 path"
  model3_path: ./saved/e2e_36_uph_uph.pth # "pretrained model2 path"
  pred_path: ./test2